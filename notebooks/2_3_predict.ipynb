{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_3_predict.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP3kGhmRCR9pea9olnufkdc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cO3ZdYJ8gVGG","executionInfo":{"status":"ok","timestamp":1620546961586,"user_tz":-540,"elapsed":715,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# masked pseudo labeling [predict]"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yfMlP8LeUBG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620547141514,"user_tz":-540,"elapsed":180637,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}},"outputId":"678672db-1dd3-480c-fc29-f82603574b61"},"source":["from google.colab import drive, files\n","import os\n","\n","drive.mount('/content/drive')  # drive をマウント\n","COLAB = \"/content/drive/MyDrive/studentcup-2021-spring\"  # colaboratory の path (必要時応じて変更)\n","os.chdir(COLAB)\n","!pip install --quiet category_encoders\n","!pip install --quiet xfeat"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","\u001b[K     |████████████████████████████████| 81kB 4.7MB/s \n","\u001b[K     |████████████████████████████████| 296kB 8.2MB/s \n","\u001b[K     |████████████████████████████████| 1.2MB 16.5MB/s \n","\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n","\u001b[K     |████████████████████████████████| 81kB 9.1MB/s \n","\u001b[K     |████████████████████████████████| 112kB 41.8MB/s \n","\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n","\u001b[K     |████████████████████████████████| 143kB 43.7MB/s \n","\u001b[?25h  Building wheel for ml-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"86RtdwEkfWQ9","executionInfo":{"status":"ok","timestamp":1620547142379,"user_tz":-540,"elapsed":181501,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["import requests\n","import os\n","\n","# make config\n","OUTPUT = os.path.join(COLAB, 'output')\n","INPUT = os.path.join(COLAB, 'input')\n","SUBMISSION = os.path.join(COLAB, 'submission')\n","EXP_NAME = \"run2\"\n","EXP = os.path.join(OUTPUT, EXP_NAME)\n","PREDS = os.path.join(EXP, \"preds\")\n","TRAINED = os.path.join(EXP, \"trained\")\n","FEATURE = os.path.join(EXP, \"feature\")\n","REPORTS = os.path.join(EXP, \"reports\")\n","\n","# make experiments environment\n","dirs = [OUTPUT,\n","        SUBMISSION,\n","        FEATURE,\n","        EXP,\n","        PREDS,\n","        TRAINED,\n","        REPORTS]\n","\n","for v in dirs:\n","    if not os.path.isdir(v):\n","        print(f\"making {v}\")\n","        os.makedirs(v)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"gT2OQtksfjIC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620547147395,"user_tz":-540,"elapsed":186510,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}},"outputId":"d95ae92e-5779-4d07-c257-14ebb6bb6511"},"source":["import datetime\n","import logging\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.figure_factory as ff\n","import joblib\n","from matplotlib_venn import venn2\n","from sklearn import model_selection\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.cluster import KMeans\n","\n","import itertools\n","\n","from lightgbm import LGBMModel\n","import category_encoders as ce\n","import xfeat\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import layers as L\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n","\n","pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rlzHOv9tWeER","executionInfo":{"status":"ok","timestamp":1620547147397,"user_tz":-540,"elapsed":186511,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# seed 固定\n","def seed_everything(seed: int):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","seed_everything(46)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"7HGSHh8XfkL8","executionInfo":{"status":"ok","timestamp":1620547147397,"user_tz":-540,"elapsed":186509,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# model save and load 用のクラス\n","class Util:\n","    @classmethod\n","    def dump(cls, value, path):\n","        os.makedirs(os.path.dirname(path), exist_ok=True)\n","        joblib.dump(value, path, compress=True)\n","\n","    @classmethod\n","    def load(cls, path):\n","        return joblib.load(path)\n","\n","# log 用のクラス\n","class Logger:\n","    def __init__(self, path):\n","        self.general_logger = logging.getLogger(path)\n","        stream_handler = logging.StreamHandler()\n","        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n","        if len(self.general_logger.handlers) == 0:\n","            self.general_logger.addHandler(stream_handler)\n","            self.general_logger.addHandler(file_general_handler)\n","            self.general_logger.setLevel(logging.INFO)\n","\n","    def info(self, message):\n","        # display time\n","        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n","\n","    @staticmethod\n","    def now_string():\n","        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n","\n","# logger の設定\n","logger = Logger(REPORTS)\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C6HhCgodfqA2"},"source":["## laod data - pseudo labeling -"]},{"cell_type":"code","metadata":{"id":"QLfXo6MTfmc0","executionInfo":{"status":"ok","timestamp":1620547148215,"user_tz":-540,"elapsed":187326,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["train = pd.read_csv(INPUT+\"/train.csv\")\n","test = pd.read_csv(INPUT+\"/test.csv\")\n","sample_sub = pd.read_csv(INPUT+\"/sample_submit.csv\")\n","genre_labels = pd.read_csv(INPUT+\"/genre_labels.csv\")"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9X7LcMP72Ht","executionInfo":{"status":"ok","timestamp":1620547149112,"user_tz":-540,"elapsed":188216,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}},"outputId":"08b6af98-4a04-4bdd-86dc-878ad6bb6710"},"source":["# pseudo labeling augmentation\n","threshold = 0.7\n","\n","path_1 =  \"output/run1/preds\"\n","preds_1 = Util.load(path_1 + \"/preds.pkl\")\n","\n","pseudo = test.copy()\n","pseudo[\"genre\"] = np.argmax(preds_1, axis=1)\n","pseudo[\"proba\"] = np.max(preds_1, axis=1)\n","\n","# cross pseudo labeling 用の flag\n","pseudo[\"flag\"] = 1\n","train[\"flag\"] = 0\n","\n","pseudo = pseudo[pseudo[\"proba\"] >= threshold][train.columns]\n","\n","# augment\n","old = len(train)\n","train = pd.concat([train, pseudo]).reset_index(drop=True)\n","new = len(train)\n","\n","logger.info(f\"Psudo Labeling : {old} -> {new}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-05-09 07:59:09] - Psudo Labeling : 4046 -> 6624\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"-DZZT0VNfsf0"},"source":["## feature engineering - load features -"]},{"cell_type":"code","metadata":{"id":"hTzhT7NOfuuQ","executionInfo":{"status":"ok","timestamp":1620547149114,"user_tz":-540,"elapsed":188217,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["class GroupingEngine: \n","\n","    def __init__(self, group_key, group_values, agg_methods):\n","        self.group_key = group_key\n","        self.group_values = group_values  # debug\n","\n","        ex_trans_methods = [\"val-mean\", \"z-score\"]\n","        self.ex_trans_methods = [m for m in agg_methods if m in ex_trans_methods]\n","        self.agg_methods = [m for m in agg_methods if m not in self.ex_trans_methods]\n","        self.df = None\n","\n","    def fit(self, input_df, y=None):\n","        if not self.agg_methods:\n","            return\n","            \n","        new_df = []\n","        for agg_method in self.agg_methods:\n","\n","            for col in self.group_values:\n","                if callable(agg_method):\n","                    agg_method_name = agg_method.__name__\n","                else:\n","                    agg_method_name = agg_method\n","\n","                new_col = f\"agg_{agg_method_name}_{col}_grpby_{self.group_key}\"\n","                df_agg = (input_df[[col] + [self.group_key]].groupby(self.group_key)[[col]].agg(agg_method))\n","                df_agg.columns = [new_col]\n","                new_df.append(df_agg)\n","        self.df = pd.concat(new_df, axis=1).reset_index()\n","\n","    def transform(self, input_df):\n","        if self.agg_methods:\n","            output_df = pd.merge(input_df[[self.group_key]], self.df, on=self.group_key, how=\"left\")\n","        else:\n","            output_df = input_df[[self.group_key]].copy()\n","\n","        if len(self.ex_trans_methods) != 0:\n","            output_df = self.ex_transform(input_df, output_df)\n","        output_df.drop(self.group_key, axis=1, inplace=True)\n","        return output_df\n","\n","    def ex_transform(self, df1, df2):\n","        \"\"\"\n","        df1: input_df\n","        df2: output_df\n","        return: output_df (added ex transformed features)\n","        \"\"\"\n","\n","        if \"val-mean\" in self.ex_trans_methods:\n","            _agg_df = xfeat.aggregation(df1, \n","                                        group_key=self.group_key,\n","                                        group_values=self.group_values, \n","                                        agg_methods=[\"mean\"])[0]\n","            df2[self._get_col(\"val-mean\")] = df1[self.group_values].values - _agg_df[self._get_col(\"mean\")].values\n","\n","        if \"z-score\" in self.ex_trans_methods:\n","            _agg_df = xfeat.aggregation(df1, \n","                                        group_key=self.group_key,\n","                                        group_values=self.group_values, \n","                                        agg_methods=[\"mean\", \"std\"])[0]\n","            df2[self._get_col(\"z-score\")] = ((df1[self.group_values].values - _agg_df[self._get_col(\"mean\")].values) \n","                                                / (_agg_df[self._get_col(\"std\")].values + 1e-8))\n","\n","        return df2\n","\n","    def _get_col(self, method):\n","        return [f\"agg_{method}_{group_val}_grpby_{self.group_key}\" for group_val in self.group_values]\n","\n","    def fit_transform(self, input_df, y=None):\n","        self.fit(input_df, y=y)\n","        return self.transform(input_df)\n","\n","\n","\n","class TargetEncodingEngine:\n","    \"\"\"\n","    refer to https://github.com/nyk510/atmacup10\n","    \"\"\"\n","\n","    def __init__(self, use_columns, cv):\n","\n","        self.mapping_df_ = None\n","        self.y_mean_ = None\n","        self.use_columns = use_columns\n","        self.cv = list(cv)\n","        self.n_fold = len(self.cv)\n","\n","    def create_mapping(self, input_df, y):\n","        self.mapping_df_ = {}\n","        self.y_mean_ = np.mean(y)\n","\n","        out_df = pd.DataFrame()\n","        target = pd.Series(y)\n","\n","        for col_name in self.use_columns:\n","            keys = input_df[col_name].unique()\n","            x = input_df[col_name]\n","\n","            oof = np.zeros_like(x, dtype=np.float)\n","\n","            for idx_train, idx_valid in self.cv:\n","                _df = target[idx_train].groupby(x[idx_train]).mean()\n","                _df = _df.reindex(keys)\n","                _df = _df.fillna(_df.mean())\n","                oof[idx_valid] = input_df[col_name][idx_valid].map(_df.to_dict())\n","\n","            out_df[col_name] = oof\n","\n","            self.mapping_df_[col_name] = target.groupby(x).mean()\n","\n","        return out_df\n","\n","    def fit(self, input_df: pd.DataFrame, y=None, **kwargs) -> None:\n","        _ = self.create_mapping(input_df, y=y)\n","\n","    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n","        out_df = pd.DataFrame()\n","\n","        for c in self.use_columns:\n","            out_df[c] = input_df[c].map(self.mapping_df_[c]).fillna(self.y_mean_)\n","\n","        return out_df.add_prefix('TE_') \n","\n","    \n","    def fit_transform(self, input_df, y=None):\n","        self.fit(input_df, y=y)\n","        return self.transform(input_df)   "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQwEX2jzfxEI","executionInfo":{"status":"ok","timestamp":1620547149496,"user_tz":-540,"elapsed":188598,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# 実際に前処理をする関数を定義\n","def get_numerical_features(input_df):\n","    # そのままの数値特徴\n","    cols = ['popularity',\n","            'duration_ms',\n","            'acousticness',\n","            'positiveness',\n","            'danceability',\n","            'loudness',\n","            'energy',\n","            'liveness',\n","            'speechiness',\n","            'instrumentalness']\n","    output_df = input_df[cols].copy()\n","    return output_df.add_prefix(\"lgb__\")\n","\n","\n","def get_ce_features(input_df):\n","    # count encording した特徴量\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = input_df.copy()\n","    _input_df[\"pop10region\"] = tmp + input_df[\"region\"]\n","\n","    cols = [\"region\", \"pop10region\"]\n","    encoder = ce.CountEncoder()\n","    output_df = encoder.fit_transform(_input_df[cols]).add_prefix(\"CE_\")\n","    return output_df.add_prefix(\"lgb__\")\n","\n","\n","def get_oe_features(input_df):\n","    # ordinal encording (label encording)した特徴量\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = input_df.copy()\n","    _input_df[\"pop10region\"] = tmp + input_df[\"region\"]\n","    cols = [\"region\", \"pop10region\"]\n","    encoder = ce.OrdinalEncoder()\n","    output_df = encoder.fit_transform(_input_df[cols]).add_prefix(\"OE_\")\n","    return output_df.add_prefix(\"lgb__\")\n","\n","\n","def get_tmpo_features(input_df):\n","    # tmpo に関する特徴量\n","    _df = input_df[\"tempo\"].str.split(\"-\").apply(pd.Series).astype(float)\n","    _df.columns = [\"tempo_low\", \"tempo_high\"]\n","    output_df = _df.copy()\n","    output_df[\"diff_tempo\"] = _df[\"tempo_high\"] - _df[\"tempo_low\"]\n","    return output_df.add_prefix(\"lgb__\")\n","\n","\n","def get_binned_popularity_features(input_df):\n","    # popularity の10の位と1の位の特徴量\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = [[i[0], i[1]] for i in tmp]\n","    output_df = pd.DataFrame(tmp, columns=[\"popularity10\", \"popularity01\"])\n","    return output_df.astype(int).add_prefix(\"lgb__\")\n","\n","# 集約特徴量作成時に使用\n","def max_min(x):\n","    return x.max() - x.min()\n","\n","def q75_q25(x):\n","    return x.quantile(0.75) - x.quantile(0.25)\n","\n","\n","def get_agg_region_features(input_df):\n","    # region をキーにした集約特徴量\n","    _input_df = pd.concat([get_tmpo_features(input_df),\n","                           input_df], axis=1)\n","    group_key = \"region\"\n","    group_values = ['popularity',\n","                    'duration_ms',\n","                    'acousticness',\n","                    'positiveness',\n","                    'danceability',\n","                    'loudness',\n","                    'energy',\n","                    'liveness',\n","                    'speechiness',\n","                    'instrumentalness', \n","                    ]\n","    agg_methods = [\"min\", \"mean\", \"max\", max_min, \"z-score\", \"var\", \"skew\", pd.DataFrame.kurt]\n","    encoder = GroupingEngine(group_key=group_key, group_values=group_values, agg_methods=agg_methods)\n","    output_df = encoder.fit_transform(_input_df)\n","    return output_df.add_prefix(\"lgb__\")\n","\n","\n","def get_agg_pop10region_features(input_df):\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = pd.concat([input_df, \n","                           get_tmpo_features(input_df)], axis=1)\n","    _input_df[\"pop10region\"] = tmp + input_df[\"region\"]\n","    group_key = \"pop10region\"\n","    group_values = ['popularity',\n","                    'duration_ms',\n","                    'acousticness',\n","                    'positiveness',\n","                    'danceability',\n","                    'loudness',\n","                    'energy',\n","                    'liveness',\n","                    'speechiness',\n","                    'instrumentalness',\n","                    ]\n","    agg_methods = [\"min\", \"mean\", \"max\", max_min, \"z-score\", \"var\", \"skew\", pd.DataFrame.kurt]\n","    encoder = GroupingEngine(group_key=group_key, group_values=group_values, agg_methods=agg_methods)\n","    output_df = encoder.fit_transform(_input_df)\n","    return output_df.add_prefix(\"lgb__\")\n","\n","def get_num_nan_features(input_df):\n","    output_df = pd.DataFrame()\n","    output_df[\"num_nan\"] = input_df.isnull().sum(axis=1)\n","    return output_df.add_prefix(\"lgb__\")\n","\n","\n","def get_target_encode_features(input_df):\n","    kf = model_selection.KFold(n_splits=10, random_state=2021, shuffle=True)\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = input_df.copy()\n","    _input_df[\"pop10region\"] = tmp + input_df[\"region\"]\n","\n","    train_df = _input_df[_input_df[\"genre\"].notnull()]\n","    train_y = ce.OneHotEncoder().fit_transform(train_df[\"genre\"].astype(str))\n","    genre = ['country',\n","             'electronic',\n","             'folk',\n","             'hip-hop',\n","             'jazz',\n","             'latin',\n","             'classic',\n","             'other-light-music',\n","             'pop',\n","             'religious',\n","             'rock']\n","    train_y.columns = genre\n","    out_lst = []\n","    for col in genre:\n","        _y = train_y[col]\n","        encoder = TargetEncodingEngine(use_columns=[\"region\", \"pop10region\"],\n","                                       cv=kf.split(train_df, _y))\n","        encoder.fit(input_df=train_df, y=_y)\n","\n","        out_df = encoder.transform(_input_df).add_suffix(f\"={col}\")\n","        out_lst.append(out_df)\n","    \n","    output_df = pd.concat(out_lst, axis=1)\n","    return output_df.add_prefix(\"lgb__\")\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1NTUXa1tBV8","executionInfo":{"status":"ok","timestamp":1620547149497,"user_tz":-540,"elapsed":188597,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# kNN features (nagiss's features)\n","def get_knn_numerical_features(input_df):\n","    cols = ['popularity',\n","            'duration_ms',\n","            'acousticness',\n","            'positiveness',\n","            'danceability',\n","            'loudness',\n","            'energy',\n","            'liveness',\n","            'speechiness',\n","            'instrumentalness']\n","\n","    output_df = input_df[cols + [\"region\"]]\n","    f = lambda x: x.fillna(x.mean())\n","    output_df = output_df.groupby('region').transform(f)\n","\n","    output_df = pd.DataFrame(StandardScaler().fit_transform(output_df), columns=cols)\n","    output_df[\"popularity8\"] = output_df[\"popularity\"] * 8\n","    return output_df.add_prefix(\"kNN__\")\n","\n","\n","def get_knn_ohe_features(input_df):\n","    # ordinal encording (label encording)した特徴量\n","\n","    cols = [\"region\"]\n","    encoder = ce.OneHotEncoder()\n","    output_df = encoder.fit_transform(input_df[cols]).add_prefix(\"OHE_\")* 100\n","    return output_df.add_prefix(\"kNN__\") \n","\n","\n","def get_knn_tmpo_features(input_df):\n","    # tmpo に関する特徴量\n","    _df = input_df[\"tempo\"].str.split(\"-\").apply(pd.Series).astype(float)\n","    _df.columns = [\"tempo_low\", \"tempo_high\"]\n","\n","    _df[\"region\"] = input_df[\"region\"].copy()\n","    f = lambda x: x.fillna(x.mean())\n","    _df = _df.groupby('region').transform(f)\n","\n","    output_df = np.log1p(_df)\n","    return output_df.add_prefix(\"kNN__\") * 0.01\n","\n","\n","def get_knn_num_nan_features(input_df):\n","    output_df = pd.DataFrame()\n","    output_df[\"num_nan\"] = input_df.drop(\"genre\", axis=1).isnull().sum(axis=1)\n","    return output_df.add_prefix(\"kNN__\") * 100\n","\n","\n","def get_knn_target_encode_features(input_df):\n","    kf = model_selection.KFold(n_splits=10, random_state=2021, shuffle=True)\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = input_df.copy()\n","    _input_df[\"pop10region\"] = tmp + input_df[\"region\"]\n","\n","    train_df = _input_df[_input_df[\"genre\"].notnull()]\n","    train_y = ce.OneHotEncoder().fit_transform(train_df[\"genre\"].astype(str))\n","    genre = ['country',\n","             'electronic',\n","             'folk',\n","             'hip-hop',\n","             'jazz',\n","             'latin',\n","             'classic',\n","             'other-light-music',\n","             'pop',\n","             'religious',\n","             'rock']\n","    train_y.columns = genre\n","    out_lst = []\n","    for col in genre:\n","        _y = train_y[col]\n","        encoder = TargetEncodingEngine(use_columns=[\"pop10region\"],\n","                                       cv=kf.split(train_df, _y))\n","        encoder.fit(input_df=train_df, y=_y)\n","\n","        out_df = encoder.transform(_input_df).add_suffix(f\"={col}\")\n","        out_lst.append(out_df)\n","    \n","    output_df = pd.concat(out_lst, axis=1).fillna(0)\n","    return output_df.add_prefix(\"kNN__\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU9H3cLZ3Dy9","executionInfo":{"status":"ok","timestamp":1620547149861,"user_tz":-540,"elapsed":188960,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# MLP\n","def get_mlp_numerical_features(input_df):\n","    # そのままの数値特徴\n","    cols = ['popularity',\n","            'duration_ms',\n","            'acousticness',\n","            'positiveness',\n","            'danceability',\n","            'loudness',\n","            'energy',\n","            'liveness',\n","            'speechiness',\n","            'instrumentalness']\n","    output_df = input_df[cols].fillna(0).copy()\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_mlp_ce_features(input_df):\n","    # count encording した特徴量\n","    _input_df = pd.concat([input_df,\n","                           get_binned_popularity_features(input_df)], axis=1).fillna(0)\n","    \n","    cols = [\"region\"]\n","    encoder = ce.CountEncoder()\n","    output_df = encoder.fit_transform(_input_df[cols]).add_prefix(\"CE_\")\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_mlp_ohe_features(input_df):\n","    # ordinal encording (label encording)した特徴量\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = input_df.fillna(0).copy()\n","    _input_df[\"pop10regeion\"] = tmp + input_df[\"region\"]\n","    cols = [\"region\", \n","            \"pop10regeion\"\n","            ]\n","    encoder = ce.OneHotEncoder()\n","    output_df = encoder.fit_transform(_input_df[cols]).add_prefix(\"OHE_\")\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_mlp_tmpo_features(input_df):\n","    # tmpo に関する特徴量\n","    _df = input_df[\"tempo\"].str.split(\"-\").apply(pd.Series).astype(float)\n","    _df.columns = [\"tempo_low\", \"tempo_high\"]\n","    output_df = _df.fillna(0).copy()\n","    output_df[\"diff_tempo\"] = _df[\"tempo_high\"] - _df[\"tempo_low\"]\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_mlp_binned_popularity_features(input_df):\n","    # popularity の10の位と1の位の特徴量\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = [[i[0], i[1]] for i in tmp]\n","    output_df = pd.DataFrame(tmp, columns=[\"popularity10\", \"popularity01\"])\n","    return output_df.astype(int).add_prefix(\"MLP__\")\n","\n","# 集約特徴量作成時に使用\n","def max_min(x):\n","    return x.max() - x.min()\n","\n","def q75_q25(x):\n","    return x.quantile(0.75) - x.quantile(0.25)\n","\n","\n","def get_mlp_agg_region_features(input_df):\n","    # region をキーにした集約特徴量\n","    _input_df = pd.concat([\n","                           get_mlp_tmpo_features(input_df),\n","                           input_df], axis=1)\n","    group_key = \"region\"\n","    group_values = ['popularity',\n","                    'duration_ms',\n","                    'acousticness',\n","                    'positiveness',\n","                    'danceability',\n","                    'loudness',\n","                    'energy',\n","                    'liveness',\n","                    'speechiness',\n","                    'instrumentalness']\n","    agg_methods = [\"z-score\",]\n","    encoder = GroupingEngine(group_key=group_key, group_values=group_values, agg_methods=agg_methods)\n","    output_df = encoder.fit_transform(_input_df).fillna(0)\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_mlp_agg_popularity10_region_features(input_df):\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = pd.concat([input_df, get_mlp_tmpo_features(input_df)], axis=1)\n","    _input_df[\"pop10regeion\"] = tmp + input_df[\"region\"]\n","    group_key = \"pop10regeion\"\n","    group_values = ['duration_ms',\n","                    'acousticness',\n","                    'positiveness',\n","                    'danceability',\n","                    'loudness',\n","                    'energy',\n","                    'liveness',\n","                    'speechiness',\n","                    'instrumentalness']\n","    agg_methods = [\"z-score\",]\n","    encoder = GroupingEngine(group_key=group_key, group_values=group_values, agg_methods=agg_methods)\n","    output_df = encoder.fit_transform(_input_df).fillna(0)\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_mlp_target_encode_features(input_df):\n","    kf = model_selection.KFold(n_splits=10, random_state=2021, shuffle=True)\n","    tmp = input_df[\"popularity\"].astype(str).str.zfill(2)\n","    tmp = pd.Series([i[0] for i in tmp])\n","    _input_df = input_df.copy()\n","    _input_df[\"pop10region\"] = tmp + input_df[\"region\"]\n","\n","    train_df = _input_df[_input_df[\"genre\"].notnull()]\n","    train_y = ce.OneHotEncoder().fit_transform(train_df[\"genre\"].astype(str))\n","    genre = ['country',\n","             'electronic',\n","             'folk',\n","             'hip-hop',\n","             'jazz',\n","             'latin',\n","             'classic',\n","             'other-light-music',\n","             'pop',\n","             'religious',\n","             'rock']\n","    train_y.columns = genre\n","    out_lst = []\n","    for col in genre:\n","        _y = train_y[col]\n","        encoder = TargetEncodingEngine(use_columns=[\"region\", \"pop10region\"],\n","                                       cv=kf.split(train_df, _y))\n","        encoder.fit(input_df=train_df, y=_y)\n","\n","        out_df = encoder.transform(_input_df).add_suffix(f\"={col}\")\n","        out_lst.append(out_df)\n","    \n","    output_df = pd.concat(out_lst, axis=1).fillna(0)\n","    return output_df.add_prefix(\"MLP__\")\n","\n","\n","def get_cross_pseudo_features(input_df):\n","    return input_df[[\"index\", \"flag\"]].copy()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"iclCu7Z3fyat","executionInfo":{"status":"ok","timestamp":1620547149862,"user_tz":-540,"elapsed":188960,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# 上で作った関数を実行し、train, test それぞれで前処理を施す関数を定義: get_train_data, get_test_data\n","def preprocess(input_df, funcs, task=\"train\"):\n","    df_lst = []\n","    for func in funcs:\n","        file_name = os.path.join(FEATURE, f\"{task}_{func.__name__}.pkl\")\n","        if os.path.isfile(file_name):\n","            _df = Util.load(file_name)\n","        else:\n","            _df = func(input_df)\n","            Util.dump(_df, file_name)\n","        df_lst.append(_df)\n","    output_df = pd.concat(df_lst, axis=1)  \n","    return output_df  \n","\n","\n","def get_test_data(train, test):\n","\n","    # whole_funcs: train+test の全体集合を対象とした処理\n","    whole_funcs = [get_numerical_features,\n","                   get_tmpo_features,\n","                   get_binned_popularity_features,get_ce_features, \n","                   get_oe_features,\n","                   get_agg_region_features,\n","                   get_agg_pop10region_features,\n","                   get_num_nan_features,\n","                   get_target_encode_features,\n","                   get_knn_numerical_features, \n","                   get_knn_ohe_features,\n","                   get_knn_tmpo_features,\n","                   get_knn_num_nan_features,\n","                   get_knn_target_encode_features,\n","                   get_mlp_numerical_features,\n","                   get_mlp_tmpo_features,\n","                   get_mlp_binned_popularity_features,\n","                   get_mlp_ce_features,\n","                   get_mlp_ohe_features,\n","                   get_mlp_agg_region_features,\n","                   get_mlp_agg_popularity10_region_features,\n","                   get_mlp_target_encode_features,\n","                   get_cross_pseudo_features]\n","\n","    whole_df = pd.concat([train, test]).reset_index(drop=True)\n","    whole_out = preprocess(whole_df, whole_funcs, task=\"whole_\")  # whole funcs による前処理\n","\n","    test_x = whole_out.iloc[len(train):].reset_index(drop=True)\n","    \n","    return test_x     "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"_sBCUI9kfzsk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620547156293,"user_tz":-540,"elapsed":195385,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}},"outputId":"f8f6c095-8b5c-4263-f040-90eaa1a09fd1"},"source":["# get features\n","test_x = get_test_data(train, test)\n","train_y = train[\"genre\"]\n","\n","print(test_x.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["(4046, 464)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mVRtSYeYf12-"},"source":["## predict"]},{"cell_type":"code","metadata":{"id":"cbOwnMQxf0-k","executionInfo":{"status":"ok","timestamp":1620547156294,"user_tz":-540,"elapsed":195385,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# set cv strategy & metrics\n","\n","# 今回は stratified k fold を使う\n","def skf(train_x, train_y, n_splits, random_state=2021, key=None):\n","    \"\"\"\n","    - make stratified k fold cv strategy\n","    - use key (stratified key)\n","    \"\"\"\n","    kf = model_selection.StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n","    return list(kf.split(train_x, train_y, key))\n","\n","def macro_f1(y_true, y_pred):\n","    y_pred = np.argmax(y_pred, axis=1)\n","    score = f1_score(y_true, y_pred, average=\"macro\")\n","    return score\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwRT7q_o7a1J","executionInfo":{"status":"ok","timestamp":1620547156294,"user_tz":-540,"elapsed":195384,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# predict 用の関数\n","def predict_cv(X, model, folds, seeds, name=\"\"):\n","    preds_seeds = []\n","\n","    for seed in seeds:\n","        preds = []\n","\n","        for i_fold in range(folds):\n","            logger.info(f\"PREDICT >>> SEED:{seed}, FOLD:{i_fold}\")\n","            model_name = os.path.join(TRAINED, f\"{name}_SEED{seed}_FOLD{i_fold}_model.pkl\")\n","            est = model()\n","            est.load(model_name)\n","            pred = est.predict(X)\n","            preds.append(pred)\n","\n","        preds = np.nanmean(preds, axis=0)\n","        preds_seeds.append(preds)\n","\n","    preds = np.nanmean(preds_seeds, axis=0)\n","    Util.dump(preds, f'{PREDS}/preds.pkl')\n","\n","    return preds\n","\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVGBZkN8igC9","executionInfo":{"status":"ok","timestamp":1620547156295,"user_tz":-540,"elapsed":195382,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# LightGBM\n","# Cross Pseudo Labeling 用に fit, predict をマイナーチェンジ\n","\n","class LGBM:\n","    def __init__(self):\n","        self.lgb = None\n","\n","        self.columns = None\n","        self.feature_importances_ = None\n","\n","        self.random_state = 2021\n","        self.learning_rate = 0.01\n","\n","        self.pseudo_idx = None\n","\n","    def build_model(self):\n","        lgb_params = {  \n","            \"n_estimators\": 10000,\n","            \"objective\": 'multiclass',\n","            \"learning_rate\": self.learning_rate,\n","            \"num_leaves\": 31,\n","            \"random_state\": self.random_state,\n","            \"n_jobs\": -1,\n","            \"importance_type\": \"gain\",\n","            \"colsample_bytree\": .5,\n","            \"n_classes\":11,\n","            \"class_weight\":\"balanced\",\n","            }\n","\n","        self.lgb = LGBMModel(**lgb_params)\n","\n","    def fit(self, tr_x, tr_y, va_x=None, va_y=None):\n","        self.build_model()\n","        \n","        x_col = tr_x.columns\n","        lgb_col = [k for k in x_col if k.startswith(\"lgb__\")]\n","        tr_lgb = tr_x[lgb_col]\n","        va_lgb = va_x[lgb_col]\n","\n","        self.columns = lgb_col\n","\n","        self.lgb.fit(tr_lgb.values, tr_y.values,\n","                     eval_set=[[va_lgb.values, va_y.values]],\n","                     early_stopping_rounds=100,\n","                     verbose=0)\n","\n","        self.feature_importances_ = self.lgb.feature_importances_\n","\n","        # train 時に使った psudo labelのレコード番号を保存\n","        self.pseudo_idx = tr_x[[\"index\", \"flag\"]].reset_index(drop=True) \n","\n","    def predict(self, x):\n","        x_col = x.columns\n","        lgb_col = [k for k in x_col if k.startswith(\"lgb__\")]\n","\n","        x_lgb = x[lgb_col]\n","        preds_lgb = self.lgb.predict(x_lgb.values)\n","\n","        # test(valid)データの予測値の中の，pseudo label で用いたものはマスク\n","        idx = self.pseudo_idx[self.pseudo_idx[\"flag\"] == 1][\"index\"].values.tolist()\n","        mask_idx = x[x[\"index\"].isin(idx)].index  # mask するindexを取得\n","        preds_lgb[mask_idx] = np.nan  # preds をmaskして同一recordの予測を隠す\n","\n","        return preds_lgb\n","\n","    def save(self, path):\n","        models = self.lgb\n","        idx = self.pseudo_idx\n","\n","        Util.dump([models, idx], path)\n","    \n","    def load(self, path):\n","        self.lgb, self.pseudo_idx = Util.load(path)\n","\n","    def get_saving_path(self, path):\n","        return path\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pzzl9bh7mlq5","executionInfo":{"status":"ok","timestamp":1620547156295,"user_tz":-540,"elapsed":195381,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["class KNN:\n","    def __init__(self):\n","        self.knn = None\n","\n","        self.random_state = None\n","        self.learning_rate = None\n","\n","        self.pseudo_idx = None\n","\n","    def build_model(self):\n","        self.knn = KNeighborsClassifier(n_jobs=-1, n_neighbors=5, weights=\"distance\")\n","\n","    def fit(self, tr_x, tr_y, va_x=None, va_y=None):\n","        self.build_model()\n","        \n","        x_col = tr_x.columns\n","        knn_col = [k for k in x_col if k.startswith(\"kNN__\")]\n","\n","        tr_knn = tr_x[knn_col]\n","        va_knn = va_x[knn_col]\n","        self.knn.fit(tr_knn.values, tr_y.values)\n","\n","        self.pseudo_idx = tr_x[[\"index\", \"flag\"]].reset_index(drop=True)\n","\n","    def predict(self, x):\n","        x_col = x.columns\n","        knn_col = [k for k in x_col if k.startswith(\"kNN__\")]\n","        x_knn = x[knn_col]\n","        preds_knn = self.knn.predict_proba(x_knn.values)\n","\n","        idx = self.pseudo_idx[self.pseudo_idx[\"flag\"] == 1][\"index\"].values.tolist()\n","        mask_idx = x[x[\"index\"].isin(idx)].index\n","        preds_knn[mask_idx] = np.nan\n","\n","        return preds_knn\n","\n","    def save(self, path):\n","        models =self.knn\n","        idx = self.pseudo_idx\n","        Util.dump([models, idx], path)\n","    \n","    def load(self, path):\n","        self.knn, self.pseudo_idx = Util.load(path)\n","\n","    def get_saving_path(self, path):\n","        return path"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"OW3908PF3X85","executionInfo":{"status":"ok","timestamp":1620547156705,"user_tz":-540,"elapsed":195789,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# MLP のラッパーモデル\n","class MLP:\n","    def __init__(self):\n","\n","        self.mlp = None\n","        self.scaler = None\n","\n","        self.columns = None\n","        self.random_state = None\n","        self.learning_rate = None\n","\n","        self.pseudo_idx = None\n","\n","    def build_model(self, input_dim=None, output_dim=None):\n","        inp = L.Input(shape=(input_dim,))\n","\n","        x = L.Dense(2 ** 10)(inp)\n","        x = L.BatchNormalization()(x)\n","        x = L.ReLU()(x)\n","        x = L.Dropout(0.2)(x)\n","\n","        x = L.Dense(2 ** 10)(x)\n","        x = L.BatchNormalization()(x)\n","        x = L.ReLU()(x)\n","        x = L.Dropout(0.2)(x)\n","\n","        x = L.Dense(2 ** 9)(x)\n","        x = L.BatchNormalization()(x)\n","        x = L.ReLU()(x)\n","        x = L.Dropout(0.2)(x)\n","\n","        out = L.Dense(output_dim, activation=\"softmax\")(x)\n","        model = tf.keras.Model(inputs=inp, outputs=out)\n","        model.compile(optimizer=\"adam\", loss='categorical_crossentropy')\n","        self.mlp = model\n","\n","    def fit(self, tr_x, tr_y, va_x=None, va_y=None):\n","        \n","        x_col = tr_x.columns\n","        mlp_col = [k for k in x_col if k.startswith(\"MLP__\")]\n","        tr_mlp = tr_x[mlp_col]\n","        va_mlp = va_x[mlp_col]\n","\n","        # standerization\n","        scaler = StandardScaler()\n","        scaler.fit(tr_mlp)\n","        tr_mlp = scaler.transform(tr_mlp)\n","        va_mlp = scaler.transform(va_mlp)\n","        self.scaler = scaler\n","\n","        # to one hot\n","        tr_y_mlp = tf.keras.utils.to_categorical(tr_y)\n","        va_y_mlp = tf.keras.utils.to_categorical(va_y)\n","\n","        self.build_model(tr_mlp.shape[1], tr_y_mlp.shape[1])\n","        self.mlp.fit(tr_mlp, tr_y_mlp,\n","                     validation_data=(va_mlp, va_y_mlp),\n","                     epochs=1024,\n","                     batch_size=512,\n","                     callbacks=[EarlyStopping('val_loss', patience=5, verbose=3, baseline=None, restore_best_weights=True),\n","                                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='min')],\n","                     verbose=0) \n","        \n","        self.pseudo_idx = tr_x[[\"index\", \"flag\"]].reset_index(drop=True)\n","\n","    def predict(self, x):\n","        x_col = x.columns\n","        mlp_col = [k for k in x_col if k.startswith(\"MLP__\")]\n","        x_mlp = x[mlp_col].values\n","        x_mlp = self.scaler.transform(x_mlp)\n","\n","        preds = self.mlp.predict(x_mlp)\n","\n","        idx = self.pseudo_idx[self.pseudo_idx[\"flag\"] == 1][\"index\"].values.tolist()\n","        mask_idx = x[x[\"index\"].isin(idx)].index\n","        preds[mask_idx] = np.nan\n","\n","        return preds\n","\n","    def save(self, path):\n","        mlp_path = f'{path}.h5'\n","        scaler_path = os.path.join(f'{path}-scaler.pkl')\n","\n","        self.mlp.save(mlp_path)\n","\n","        idx = self.pseudo_idx  # psuedo index も保存\n","        Util.dump([self.scaler, idx], scaler_path)\n","    \n","    def load(self, path):\n","        mlp_path = os.path.join(f'{path}.h5')\n","        scaler_path = os.path.join(f'{path}-scaler.pkl')\n","        self.mlp = load_model(mlp_path)\n","        self.scaler, self.pseudo_idx = Util.load(scaler_path)\n","\n","    def get_saving_path(self, path):\n","        return os.path.join(f'{path}.h5')\n"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsZZjRE2f7JK","executionInfo":{"status":"ok","timestamp":1620547156706,"user_tz":-540,"elapsed":195789,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# stratified k fold のキーを設定\n","cv_key = train_y.astype(str) + train[\"region\"] + train[\"flag\"].astype(str)\n","\n","# run parameter\n","folds = 10\n","seeds = range(5)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"-f1PjdwKf_yj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620547644175,"user_tz":-540,"elapsed":683253,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}},"outputId":"98661727-e338-435e-8a9c-769a23f93f68"},"source":["# predict \n","preds_01 = predict_cv(X=test_x, model=LGBM, folds=folds, seeds=seeds, name=\"LGBM\")\n","preds_02 = predict_cv(X=test_x, model=KNN, folds=folds, seeds=seeds, name=\"KNN\")\n","preds_03 = predict_cv(X=test_x, model=MLP, folds=folds, seeds=seeds, name=\"MLP\")"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[2021-05-09 07:59:17] - PREDICT >>> SEED:0, FOLD:0\n","[2021-05-09 07:59:21] - PREDICT >>> SEED:0, FOLD:1\n","[2021-05-09 07:59:26] - PREDICT >>> SEED:0, FOLD:2\n","[2021-05-09 07:59:31] - PREDICT >>> SEED:0, FOLD:3\n","[2021-05-09 07:59:36] - PREDICT >>> SEED:0, FOLD:4\n","[2021-05-09 07:59:40] - PREDICT >>> SEED:0, FOLD:5\n","[2021-05-09 07:59:45] - PREDICT >>> SEED:0, FOLD:6\n","[2021-05-09 07:59:50] - PREDICT >>> SEED:0, FOLD:7\n","[2021-05-09 07:59:55] - PREDICT >>> SEED:0, FOLD:8\n","[2021-05-09 08:00:00] - PREDICT >>> SEED:0, FOLD:9\n","[2021-05-09 08:00:05] - PREDICT >>> SEED:1, FOLD:0\n","[2021-05-09 08:00:10] - PREDICT >>> SEED:1, FOLD:1\n","[2021-05-09 08:00:15] - PREDICT >>> SEED:1, FOLD:2\n","[2021-05-09 08:00:20] - PREDICT >>> SEED:1, FOLD:3\n","[2021-05-09 08:00:24] - PREDICT >>> SEED:1, FOLD:4\n","[2021-05-09 08:00:29] - PREDICT >>> SEED:1, FOLD:5\n","[2021-05-09 08:00:33] - PREDICT >>> SEED:1, FOLD:6\n","[2021-05-09 08:00:38] - PREDICT >>> SEED:1, FOLD:7\n","[2021-05-09 08:00:43] - PREDICT >>> SEED:1, FOLD:8\n","[2021-05-09 08:00:48] - PREDICT >>> SEED:1, FOLD:9\n","[2021-05-09 08:00:52] - PREDICT >>> SEED:2, FOLD:0\n","[2021-05-09 08:00:56] - PREDICT >>> SEED:2, FOLD:1\n","[2021-05-09 08:01:01] - PREDICT >>> SEED:2, FOLD:2\n","[2021-05-09 08:01:06] - PREDICT >>> SEED:2, FOLD:3\n","[2021-05-09 08:01:11] - PREDICT >>> SEED:2, FOLD:4\n","[2021-05-09 08:01:16] - PREDICT >>> SEED:2, FOLD:5\n","[2021-05-09 08:01:21] - PREDICT >>> SEED:2, FOLD:6\n","[2021-05-09 08:01:26] - PREDICT >>> SEED:2, FOLD:7\n","[2021-05-09 08:01:31] - PREDICT >>> SEED:2, FOLD:8\n","[2021-05-09 08:01:37] - PREDICT >>> SEED:2, FOLD:9\n","[2021-05-09 08:01:43] - PREDICT >>> SEED:3, FOLD:0\n","[2021-05-09 08:01:50] - PREDICT >>> SEED:3, FOLD:1\n","[2021-05-09 08:01:55] - PREDICT >>> SEED:3, FOLD:2\n","[2021-05-09 08:02:00] - PREDICT >>> SEED:3, FOLD:3\n","[2021-05-09 08:02:05] - PREDICT >>> SEED:3, FOLD:4\n","[2021-05-09 08:02:11] - PREDICT >>> SEED:3, FOLD:5\n","[2021-05-09 08:02:15] - PREDICT >>> SEED:3, FOLD:6\n","[2021-05-09 08:02:19] - PREDICT >>> SEED:3, FOLD:7\n","[2021-05-09 08:02:24] - PREDICT >>> SEED:3, FOLD:8\n","[2021-05-09 08:02:28] - PREDICT >>> SEED:3, FOLD:9\n","[2021-05-09 08:02:33] - PREDICT >>> SEED:4, FOLD:0\n","[2021-05-09 08:02:39] - PREDICT >>> SEED:4, FOLD:1\n","[2021-05-09 08:02:44] - PREDICT >>> SEED:4, FOLD:2\n","[2021-05-09 08:02:49] - PREDICT >>> SEED:4, FOLD:3\n","[2021-05-09 08:02:54] - PREDICT >>> SEED:4, FOLD:4\n","[2021-05-09 08:02:59] - PREDICT >>> SEED:4, FOLD:5\n","[2021-05-09 08:03:04] - PREDICT >>> SEED:4, FOLD:6\n","[2021-05-09 08:03:09] - PREDICT >>> SEED:4, FOLD:7\n","[2021-05-09 08:03:14] - PREDICT >>> SEED:4, FOLD:8\n","[2021-05-09 08:03:20] - PREDICT >>> SEED:4, FOLD:9\n","[2021-05-09 08:03:25] - PREDICT >>> SEED:0, FOLD:0\n","[2021-05-09 08:03:27] - PREDICT >>> SEED:0, FOLD:1\n","[2021-05-09 08:03:28] - PREDICT >>> SEED:0, FOLD:2\n","[2021-05-09 08:03:30] - PREDICT >>> SEED:0, FOLD:3\n","[2021-05-09 08:03:31] - PREDICT >>> SEED:0, FOLD:4\n","[2021-05-09 08:03:33] - PREDICT >>> SEED:0, FOLD:5\n","[2021-05-09 08:03:35] - PREDICT >>> SEED:0, FOLD:6\n","[2021-05-09 08:03:36] - PREDICT >>> SEED:0, FOLD:7\n","[2021-05-09 08:03:38] - PREDICT >>> SEED:0, FOLD:8\n","[2021-05-09 08:03:39] - PREDICT >>> SEED:0, FOLD:9\n","[2021-05-09 08:03:40] - PREDICT >>> SEED:1, FOLD:0\n","[2021-05-09 08:03:42] - PREDICT >>> SEED:1, FOLD:1\n","[2021-05-09 08:03:44] - PREDICT >>> SEED:1, FOLD:2\n","[2021-05-09 08:03:45] - PREDICT >>> SEED:1, FOLD:3\n","[2021-05-09 08:03:47] - PREDICT >>> SEED:1, FOLD:4\n","[2021-05-09 08:03:48] - PREDICT >>> SEED:1, FOLD:5\n","[2021-05-09 08:03:50] - PREDICT >>> SEED:1, FOLD:6\n","[2021-05-09 08:03:51] - PREDICT >>> SEED:1, FOLD:7\n","[2021-05-09 08:03:53] - PREDICT >>> SEED:1, FOLD:8\n","[2021-05-09 08:03:54] - PREDICT >>> SEED:1, FOLD:9\n","[2021-05-09 08:03:56] - PREDICT >>> SEED:2, FOLD:0\n","[2021-05-09 08:03:57] - PREDICT >>> SEED:2, FOLD:1\n","[2021-05-09 08:03:59] - PREDICT >>> SEED:2, FOLD:2\n","[2021-05-09 08:04:00] - PREDICT >>> SEED:2, FOLD:3\n","[2021-05-09 08:04:01] - PREDICT >>> SEED:2, FOLD:4\n","[2021-05-09 08:04:03] - PREDICT >>> SEED:2, FOLD:5\n","[2021-05-09 08:04:05] - PREDICT >>> SEED:2, FOLD:6\n","[2021-05-09 08:04:06] - PREDICT >>> SEED:2, FOLD:7\n","[2021-05-09 08:04:08] - PREDICT >>> SEED:2, FOLD:8\n","[2021-05-09 08:04:09] - PREDICT >>> SEED:2, FOLD:9\n","[2021-05-09 08:04:11] - PREDICT >>> SEED:3, FOLD:0\n","[2021-05-09 08:04:12] - PREDICT >>> SEED:3, FOLD:1\n","[2021-05-09 08:04:14] - PREDICT >>> SEED:3, FOLD:2\n","[2021-05-09 08:04:16] - PREDICT >>> SEED:3, FOLD:3\n","[2021-05-09 08:04:17] - PREDICT >>> SEED:3, FOLD:4\n","[2021-05-09 08:04:19] - PREDICT >>> SEED:3, FOLD:5\n","[2021-05-09 08:04:20] - PREDICT >>> SEED:3, FOLD:6\n","[2021-05-09 08:04:22] - PREDICT >>> SEED:3, FOLD:7\n","[2021-05-09 08:04:23] - PREDICT >>> SEED:3, FOLD:8\n","[2021-05-09 08:04:25] - PREDICT >>> SEED:3, FOLD:9\n","[2021-05-09 08:04:26] - PREDICT >>> SEED:4, FOLD:0\n","[2021-05-09 08:04:28] - PREDICT >>> SEED:4, FOLD:1\n","[2021-05-09 08:04:30] - PREDICT >>> SEED:4, FOLD:2\n","[2021-05-09 08:04:31] - PREDICT >>> SEED:4, FOLD:3\n","[2021-05-09 08:04:33] - PREDICT >>> SEED:4, FOLD:4\n","[2021-05-09 08:04:34] - PREDICT >>> SEED:4, FOLD:5\n","[2021-05-09 08:04:36] - PREDICT >>> SEED:4, FOLD:6\n","[2021-05-09 08:04:38] - PREDICT >>> SEED:4, FOLD:7\n","[2021-05-09 08:04:39] - PREDICT >>> SEED:4, FOLD:8\n","[2021-05-09 08:04:40] - PREDICT >>> SEED:4, FOLD:9\n","[2021-05-09 08:04:42] - PREDICT >>> SEED:0, FOLD:0\n","[2021-05-09 08:04:47] - PREDICT >>> SEED:0, FOLD:1\n","[2021-05-09 08:04:50] - PREDICT >>> SEED:0, FOLD:2\n","[2021-05-09 08:04:54] - PREDICT >>> SEED:0, FOLD:3\n","[2021-05-09 08:04:57] - PREDICT >>> SEED:0, FOLD:4\n","[2021-05-09 08:05:00] - PREDICT >>> SEED:0, FOLD:5\n","[2021-05-09 08:05:04] - PREDICT >>> SEED:0, FOLD:6\n","[2021-05-09 08:05:06] - PREDICT >>> SEED:0, FOLD:7\n","[2021-05-09 08:05:10] - PREDICT >>> SEED:0, FOLD:8\n","[2021-05-09 08:05:13] - PREDICT >>> SEED:0, FOLD:9\n","[2021-05-09 08:05:16] - PREDICT >>> SEED:1, FOLD:0\n","[2021-05-09 08:05:19] - PREDICT >>> SEED:1, FOLD:1\n","[2021-05-09 08:05:23] - PREDICT >>> SEED:1, FOLD:2\n","[2021-05-09 08:05:26] - PREDICT >>> SEED:1, FOLD:3\n","[2021-05-09 08:05:29] - PREDICT >>> SEED:1, FOLD:4\n","[2021-05-09 08:05:33] - PREDICT >>> SEED:1, FOLD:5\n","[2021-05-09 08:05:36] - PREDICT >>> SEED:1, FOLD:6\n","[2021-05-09 08:05:40] - PREDICT >>> SEED:1, FOLD:7\n","[2021-05-09 08:05:44] - PREDICT >>> SEED:1, FOLD:8\n","[2021-05-09 08:05:47] - PREDICT >>> SEED:1, FOLD:9\n","[2021-05-09 08:05:50] - PREDICT >>> SEED:2, FOLD:0\n","[2021-05-09 08:05:53] - PREDICT >>> SEED:2, FOLD:1\n","[2021-05-09 08:05:56] - PREDICT >>> SEED:2, FOLD:2\n","[2021-05-09 08:05:59] - PREDICT >>> SEED:2, FOLD:3\n","[2021-05-09 08:06:02] - PREDICT >>> SEED:2, FOLD:4\n","[2021-05-09 08:06:06] - PREDICT >>> SEED:2, FOLD:5\n","[2021-05-09 08:06:09] - PREDICT >>> SEED:2, FOLD:6\n","[2021-05-09 08:06:11] - PREDICT >>> SEED:2, FOLD:7\n","[2021-05-09 08:06:15] - PREDICT >>> SEED:2, FOLD:8\n","[2021-05-09 08:06:18] - PREDICT >>> SEED:2, FOLD:9\n","[2021-05-09 08:06:21] - PREDICT >>> SEED:3, FOLD:0\n","[2021-05-09 08:06:25] - PREDICT >>> SEED:3, FOLD:1\n","[2021-05-09 08:06:28] - PREDICT >>> SEED:3, FOLD:2\n","[2021-05-09 08:06:30] - PREDICT >>> SEED:3, FOLD:3\n","[2021-05-09 08:06:34] - PREDICT >>> SEED:3, FOLD:4\n","[2021-05-09 08:06:36] - PREDICT >>> SEED:3, FOLD:5\n","[2021-05-09 08:06:40] - PREDICT >>> SEED:3, FOLD:6\n","[2021-05-09 08:06:42] - PREDICT >>> SEED:3, FOLD:7\n","[2021-05-09 08:06:46] - PREDICT >>> SEED:3, FOLD:8\n","[2021-05-09 08:06:49] - PREDICT >>> SEED:3, FOLD:9\n","[2021-05-09 08:06:52] - PREDICT >>> SEED:4, FOLD:0\n","[2021-05-09 08:06:55] - PREDICT >>> SEED:4, FOLD:1\n","[2021-05-09 08:06:58] - PREDICT >>> SEED:4, FOLD:2\n","[2021-05-09 08:07:02] - PREDICT >>> SEED:4, FOLD:3\n","[2021-05-09 08:07:05] - PREDICT >>> SEED:4, FOLD:4\n","[2021-05-09 08:07:09] - PREDICT >>> SEED:4, FOLD:5\n","[2021-05-09 08:07:12] - PREDICT >>> SEED:4, FOLD:6\n","[2021-05-09 08:07:16] - PREDICT >>> SEED:4, FOLD:7\n","[2021-05-09 08:07:19] - PREDICT >>> SEED:4, FOLD:8\n","[2021-05-09 08:07:21] - PREDICT >>> SEED:4, FOLD:9\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YkzDSBMC7syk"},"source":["## stacking"]},{"cell_type":"code","metadata":{"id":"5S7isSo77BQb","executionInfo":{"status":"ok","timestamp":1620547644178,"user_tz":-540,"elapsed":683254,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# simple stacking \n","class MLP2(MLP):\n","    def build_model(self, input_dim=None, output_dim=None):\n","        inp = L.Input(shape=(input_dim,))\n","\n","        x = L.Dense(2 ** 6)(inp)\n","        x = L.BatchNormalization()(x)\n","        x = L.ReLU()(x)\n","        x = L.Dropout(0.1)(x)\n","\n","        x = L.Dense(2 ** 6)(x)\n","        x = L.BatchNormalization()(x)\n","        x = L.ReLU()(x)\n","        x = L.Dropout(0.1)(x)\n","\n","        x = L.Dense(2 ** 6)(x)\n","        x = L.BatchNormalization()(x)\n","        x = L.ReLU()(x)\n","        x = L.Dropout(0.1)(x)\n","\n","        out = L.Dense(output_dim, activation=\"softmax\")(x)\n","        model = tf.keras.Model(inputs=inp, outputs=out)\n","        model.compile(optimizer=\"adam\", loss='categorical_crossentropy')\n","        self.mlp = model"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBI-C0y-7RkC","executionInfo":{"status":"ok","timestamp":1620547644178,"user_tz":-540,"elapsed":683253,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["test_xs = np.hstack([preds_01, preds_02, preds_03])\n","test_xs = pd.DataFrame(test_xs).add_prefix(\"MLP__\")\n","\n","# masked pseudo labeling augmentation\n","test_xs[\"index\"] = test_x[\"index\"]\n","test_xs[\"flag\"] = test_x[\"flag\"]"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"iz4gzvAZ7oIv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620547692732,"user_tz":-540,"elapsed":731802,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}},"outputId":"ef42e932-8b5b-4ec9-ac56-3c7bf09dd11f"},"source":["preds_04 = predict_cv(X=test_xs, model=MLP2, folds=folds, seeds=seeds, name=\"MLP2\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[2021-05-09 08:07:24] - PREDICT >>> SEED:0, FOLD:0\n","[2021-05-09 08:07:25] - PREDICT >>> SEED:0, FOLD:1\n","[2021-05-09 08:07:26] - PREDICT >>> SEED:0, FOLD:2\n","[2021-05-09 08:07:27] - PREDICT >>> SEED:0, FOLD:3\n","[2021-05-09 08:07:28] - PREDICT >>> SEED:0, FOLD:4\n","[2021-05-09 08:07:29] - PREDICT >>> SEED:0, FOLD:5\n","[2021-05-09 08:07:30] - PREDICT >>> SEED:0, FOLD:6\n","[2021-05-09 08:07:31] - PREDICT >>> SEED:0, FOLD:7\n","[2021-05-09 08:07:32] - PREDICT >>> SEED:0, FOLD:8\n","[2021-05-09 08:07:33] - PREDICT >>> SEED:0, FOLD:9\n","[2021-05-09 08:07:34] - PREDICT >>> SEED:1, FOLD:0\n","[2021-05-09 08:07:35] - PREDICT >>> SEED:1, FOLD:1\n","[2021-05-09 08:07:36] - PREDICT >>> SEED:1, FOLD:2\n","[2021-05-09 08:07:37] - PREDICT >>> SEED:1, FOLD:3\n","[2021-05-09 08:07:38] - PREDICT >>> SEED:1, FOLD:4\n","[2021-05-09 08:07:39] - PREDICT >>> SEED:1, FOLD:5\n","[2021-05-09 08:07:40] - PREDICT >>> SEED:1, FOLD:6\n","[2021-05-09 08:07:41] - PREDICT >>> SEED:1, FOLD:7\n","[2021-05-09 08:07:42] - PREDICT >>> SEED:1, FOLD:8\n","[2021-05-09 08:07:43] - PREDICT >>> SEED:1, FOLD:9\n","[2021-05-09 08:07:44] - PREDICT >>> SEED:2, FOLD:0\n","[2021-05-09 08:07:45] - PREDICT >>> SEED:2, FOLD:1\n","[2021-05-09 08:07:46] - PREDICT >>> SEED:2, FOLD:2\n","[2021-05-09 08:07:47] - PREDICT >>> SEED:2, FOLD:3\n","[2021-05-09 08:07:48] - PREDICT >>> SEED:2, FOLD:4\n","[2021-05-09 08:07:48] - PREDICT >>> SEED:2, FOLD:5\n","[2021-05-09 08:07:49] - PREDICT >>> SEED:2, FOLD:6\n","[2021-05-09 08:07:51] - PREDICT >>> SEED:2, FOLD:7\n","[2021-05-09 08:07:51] - PREDICT >>> SEED:2, FOLD:8\n","[2021-05-09 08:07:52] - PREDICT >>> SEED:2, FOLD:9\n","[2021-05-09 08:07:53] - PREDICT >>> SEED:3, FOLD:0\n","[2021-05-09 08:07:54] - PREDICT >>> SEED:3, FOLD:1\n","[2021-05-09 08:07:55] - PREDICT >>> SEED:3, FOLD:2\n","[2021-05-09 08:07:56] - PREDICT >>> SEED:3, FOLD:3\n","[2021-05-09 08:07:57] - PREDICT >>> SEED:3, FOLD:4\n","[2021-05-09 08:07:58] - PREDICT >>> SEED:3, FOLD:5\n","[2021-05-09 08:07:59] - PREDICT >>> SEED:3, FOLD:6\n","[2021-05-09 08:08:00] - PREDICT >>> SEED:3, FOLD:7\n","[2021-05-09 08:08:01] - PREDICT >>> SEED:3, FOLD:8\n","[2021-05-09 08:08:02] - PREDICT >>> SEED:3, FOLD:9\n","[2021-05-09 08:08:03] - PREDICT >>> SEED:4, FOLD:0\n","[2021-05-09 08:08:04] - PREDICT >>> SEED:4, FOLD:1\n","[2021-05-09 08:08:05] - PREDICT >>> SEED:4, FOLD:2\n","[2021-05-09 08:08:06] - PREDICT >>> SEED:4, FOLD:3\n","[2021-05-09 08:08:07] - PREDICT >>> SEED:4, FOLD:4\n","[2021-05-09 08:08:08] - PREDICT >>> SEED:4, FOLD:5\n","[2021-05-09 08:08:09] - PREDICT >>> SEED:4, FOLD:6\n","[2021-05-09 08:08:10] - PREDICT >>> SEED:4, FOLD:7\n","[2021-05-09 08:08:11] - PREDICT >>> SEED:4, FOLD:8\n","[2021-05-09 08:08:12] - PREDICT >>> SEED:4, FOLD:9\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"60A-ioLc9tJ4"},"source":["## weighted average"]},{"cell_type":"code","metadata":{"id":"whQ1gUyvBcsS","executionInfo":{"status":"ok","timestamp":1620547693079,"user_tz":-540,"elapsed":732148,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["# simple optimization\n","from scipy.optimize import minimize\n","\n","def f(w, y_true, y_preds):\n","    y = np.average(y_preds, weights=w, axis=0)\n","    y = np.argmax(y, axis=1)\n","    score = f1_score(y_true, y, average=\"macro\")\n","    return -score\n","\n","def opt_weight(train_y=None, oof=None):\n","    file_name = os.path.join(TRAINED, \"optimized_weights.pkl\")\n","    if os.path.isfile(file_name):\n","        w = Util.load(file_name)\n","    else:\n","        w0 = np.array([0.05] * len(oof))\n","        res = minimize(f, w0, args=(train_y, oof), method='Powell')\n","        w = res[\"x\"]\n","        print(res)\n","        Util.dump(w, file_name)\n","    return w\n","\n","# get data\n","preds = [preds_04, preds_01, preds_02, preds_03]\n","\n","# optim weight\n","opt_w = opt_weight()  # load weight\n","preds = np.average(preds, weights=opt_w, axis=0)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6NxuTgB1gIpc"},"source":["## make submission"]},{"cell_type":"code","metadata":{"id":"cz-syiKhgHS_","executionInfo":{"status":"ok","timestamp":1620547693080,"user_tz":-540,"elapsed":732148,"user":{"displayName":"鳥羽真仁","photoUrl":"","userId":"00518710488487886706"}}},"source":["sub = test[[\"index\"]]\n","sub[\"genre\"] = np.argmax(preds, axis=1)\n","sub.to_csv(f\"{SUBMISSION}/{EXP_NAME}.csv\", index=None, header=None)  # ファイル名を cv のスコアにするのもいいかもです"],"execution_count":26,"outputs":[]}]}